{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import math \n",
    "from pathlib import Path\n",
    "import copy\n",
    "import operator\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam# use ADAM if it doesnt work\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to import data from folder `IM1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_patients(subjects, ratios):\n",
    "\n",
    "    number_of_patients = len(subjects)\n",
    "    train_size = int(number_of_patients*ratios[0])\n",
    "    test_size = number_of_patients - train_size\n",
    "\n",
    "    val_size = int(train_size*ratios[1])\n",
    "    train_size -= val_size\n",
    "\n",
    "    return train_size, val_size, test_size\n",
    "\n",
    "\n",
    "def process_subjects(list_of_names, table):\n",
    "    pixel_data = []\n",
    "    labels = []\n",
    "\n",
    "    for name in list_of_names:\n",
    "        matching_files = table.loc[table['name'] == name]\n",
    "\n",
    "        for _, row in matching_files.iterrows():\n",
    "            filename = row['filename']\n",
    "            label = row['label']\n",
    "            try:\n",
    "                img = Image.open(\"/Shared/CipacProcessing/Projects/RF_ML_ENA1/IM1/\"+f\"{filename}\").convert('L')  # Convert image to grayscale\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "            \n",
    "            if img.size == (1024, 1024):\n",
    "                # Apply histogram equalization\n",
    "                img = cv2.equalizeHist(np.array(img))\n",
    "                # Apply gamma correction\n",
    "                img = np.array(255 * (img / 255) ** 0.7, dtype=np.uint8) # PLAY WITH THIS (0.85) - lower is lighter image\n",
    "                pixel_array = np.array(img)               \n",
    "                #min-max normalization\n",
    "                min_value = np.min(pixel_array)\n",
    "                max_value = np.max(pixel_array)\n",
    "                pixel_array = (pixel_array - min_value) / (max_value - min_value + 1e-12)\n",
    "                \n",
    "                pixel_data.append(pixel_array.reshape(pixel_array.shape[0], pixel_array.shape[1], 1))\n",
    "                labels.append(label)\n",
    "\n",
    "    labels_array = np.array([0 if label.lower() == \"oc\" else 1 for label in labels])\n",
    "    pixel_array = np.stack(pixel_data, axis=0)  # Stack arrays along a new dimension\n",
    "    \n",
    "    return pixel_array, labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = glob.glob(\"/Shared/CipacProcessing/Projects/RF_ML_ENA1/IM1/*.jpg\")\n",
    "print(len(list_files))\n",
    "table_metadata = pd.read_csv(\"/Shared/CipacProcessing/Projects/RF_ML_ENA1/table.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = set()\n",
    "\n",
    "for s in table_metadata[\"name\"]:\n",
    "    subjects.add(s)\n",
    "\n",
    "print(\"Number of patients:\", len(subjects))\n",
    "\n",
    "np.random.seed(42) #Random seed, select if you want to keep same randomization\n",
    "subject_list = list(subjects)\n",
    "np.random.shuffle(subject_list)\n",
    "\n",
    "ratios = [0.8, 0.2]#train/val\n",
    "train_size, val_size, test_size = split_patients(subjects, ratios)\n",
    "\n",
    "subject_train_set = subject_list[:train_size]\n",
    "subject_val_set = subject_list[train_size:train_size+val_size]\n",
    "subject_test_set = subject_list[train_size+val_size:]\n",
    "\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Val size:\", val_size)\n",
    "print(\"Test size:\", test_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the respective Train/Val/Test `pixel arrays` and `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_array_train, labels_train = process_subjects(subject_train_set, table_metadata)\n",
    "pixel_array_val, labels_val = process_subjects(subject_val_set, table_metadata)\n",
    "pixel_array_test, labels_test = process_subjects(subject_test_set, table_metadata)\n",
    "\n",
    "# Validate the shape size\n",
    "print(pixel_array_train.shape, labels_train.shape)\n",
    "print(pixel_array_val.shape, labels_val.shape)\n",
    "print(pixel_array_test.shape, labels_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell to print out images to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title(labels_train[22])\n",
    "plt.imshow(pixel_array_train[22], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(labels_train[23])\n",
    "plt.imshow(pixel_array_train[23], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation for Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator( # PLAY WITH THESE\n",
    "    rotation_range=1,  # Rotate image by up to 10 degrees\n",
    "    width_shift_range=0.1,  # Shift image horizontally by up to 5% of the image size\n",
    "    height_shift_range=0.1,  # Shift image vertically by up to 5% of the image size\n",
    "    zoom_range=0.1,  # Zoom in or out by up to 5%\n",
    "    horizontal_flip=True,  # Flip image horizontally\n",
    "    # vertical_flip=True,\n",
    "    # fill_mode='nearest',  # Fill missing pixels with the nearest value\n",
    "    # preprocessing_function=preprocess_input  # Preprocess input using ResNet50 preprocessing\n",
    ")\n",
    "\n",
    "# no augmentation for val set\n",
    "datagen_val = ImageDataGenerator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=(1024, 1024, 1))\n",
    "\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(input_A)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "conv3 = Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "\n",
    "#fourth layer added - play with this, can remove too if needed\n",
    "conv4 = Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "\n",
    "fc = keras.layers.Flatten()(pool4)\n",
    "fc = keras.layers.Dense(128, activation=\"relu\")(fc)\n",
    "fc = tf.keras.layers.Dropout(0.55)(fc) #0.5 - PLAY WITH DROPOUT LAYER\n",
    "fc = keras.layers.Dense(1, activation=\"sigmoid\")(fc)\n",
    "\n",
    "model = keras.Model(inputs=[input_A], outputs=[fc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = legacy.Adam(lr=1e-4) #1e-4 - CAN PLAY WITH LEARN RATE\n",
    "\n",
    "# Define the callbacks\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='auto')\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "\n",
    "model_chck_point_path = \"/Shared/CipacProcessing/Projects/RF_ML_ENA1/my_models/Final_models/m6.h5\" # CHANGE FILE NAME for NEW MODEL\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_chck_point_path, mode = 'auto', monitor='val_loss',verbose=0, save_best_only=True)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and save best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_ = 52\n",
    "hist = model.fit(datagen.flow(pixel_array_train, labels_train, batch_size=batch_size_, shuffle=True), \n",
    "                 steps_per_epoch=len(pixel_array_train) // batch_size_,\n",
    "                 epochs=100, \n",
    "                 verbose=1, \n",
    "                 validation_data=datagen_val.flow(pixel_array_val, labels_val, batch_size=24),\n",
    "                 callbacks=[checkpoint, reduce_lr,earlyStopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = keras.models.load_model(model_chck_point_path)\n",
    "test_loss, test_acc = model_load.evaluate(pixel_array_test, labels_test, verbose=0)\n",
    "\n",
    "y_pred = model_load.predict(pixel_array_test)\n",
    "binary_predictions = np.round(y_pred)\n",
    "\n",
    "f1 = f1_score(labels_test, binary_predictions)\n",
    "\n",
    "accuracy = accuracy_score(labels_test, binary_predictions)\n",
    "\n",
    "print(\"score using .evaluate():\",test_acc)\n",
    "print(\"accuracy score:\",accuracy)\n",
    "print(\"f1-score:\",f1)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "report = classification_report(labels_test, binary_predictions)\n",
    "print (report)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "matrix = confusion_matrix(labels_test, binary_predictions)\n",
    "print (matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['occluded', 'recanalized']\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(matrix, display_labels=class_names)\n",
    "cmd.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(hist.history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(hist.history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VAL - same steps as above repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pixel_array = np.concatenate((pixel_array_train, pixel_array_val, pixel_array_test), axis=0)\n",
    "# full_label_array = np.concatenate((labels_train, labels_val, labels_test), axis=0)\n",
    "# print(full_pixel_array.shape,full_label_array.shape)\n",
    "full_pixel_array = np.concatenate((pixel_array_train, pixel_array_val), axis=0)\n",
    "full_label_array = np.concatenate((labels_train, labels_val), axis=0)\n",
    "print(full_pixel_array.shape,full_label_array.shape)\n",
    "print(pixel_array_val.shape,labels_val.shape)\n",
    "print(pixel_array_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "pixel_array_splits = np.array_split(full_pixel_array, num_splits)\n",
    "label_array_splits = np.array_split(full_label_array, num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "num_splits = 5\n",
    "scores_acc_eval = []\n",
    "scores_acc_acc = []\n",
    "scores_f1 = []\n",
    "classification_reports = []\n",
    "confusion_matrices = []\n",
    "history_list = []\n",
    "\n",
    "# Change folder name as needed\n",
    "model_folder = \"/Shared/CipacProcessing/Projects/RF_ML_ENA1/my_models/Final_models\"\n",
    "model_basename = \"best_mod\"\n",
    "\n",
    "for val_index in range(num_splits):\n",
    "    # select one split as validation set\n",
    "    val_pixel_array = pixel_array_splits[val_index]\n",
    "    val_label_array = label_array_splits[val_index]\n",
    "\n",
    "    # combine remaining splits into training set\n",
    "    train_pixel_array = np.concatenate(\n",
    "        [split for i, split in enumerate(pixel_array_splits) if i != val_index],\n",
    "        axis=0\n",
    "    )\n",
    "    train_label_array = np.concatenate(\n",
    "        [split for i, split in enumerate(label_array_splits) if i != val_index],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    # Shuffle training set and corresponding indices\n",
    "    train_shuffle_indices = np.random.permutation(len(train_pixel_array))\n",
    "    train_pixel_array = train_pixel_array[train_shuffle_indices]\n",
    "    train_label_array = train_label_array[train_shuffle_indices]\n",
    "\n",
    "    # Shuffle validation set and corresponding indices\n",
    "    val_shuffle_indices = np.random.permutation(len(val_pixel_array))\n",
    "    val_pixel_array = val_pixel_array[val_shuffle_indices]\n",
    "    val_label_array = val_label_array[val_shuffle_indices]\n",
    "\n",
    "    # Shuffle test set and corresponding indices\n",
    "    test_shuffle_indices = np.random.permutation(len(pixel_array_test))\n",
    "    test_pixel_array = pixel_array_test[test_shuffle_indices]\n",
    "    test_label_array = labels_test[test_shuffle_indices]\n",
    "\n",
    "    # Define the data generators for this fold\n",
    "    train_datagen = datagen.flow(train_pixel_array, train_label_array, batch_size=32, shuffle=True)\n",
    "    val_datagen = datagen_val.flow(val_pixel_array, val_label_array, batch_size=32)\n",
    "    \n",
    "    opt = legacy.Adam(lr=3e-4)\n",
    "\n",
    "    # Define the callbacks\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=6, min_lr=1e-8)\n",
    "    # model_chck_point_path = \"/Shared/CipacProcessing/Projects/RF_ML_ENA1/my_models/mymodelfin5.h5\"\n",
    "    model_chck_point_path = f\"{model_folder}{model_basename}_split{val_index+1}.h5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_chck_point_path, mode = 'auto', monitor='val_loss',verbose=0, save_best_only=True)\n",
    "\n",
    "    # Define and compile the model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Train the model for this fold\n",
    "    hist = model.fit(train_datagen, \n",
    "                     steps_per_epoch=len(train_datagen),\n",
    "                     epochs=45, \n",
    "                     verbose=1, \n",
    "                     validation_data=val_datagen,\n",
    "                     callbacks=[checkpoint, reduce_lr])\n",
    "    \n",
    "    history_list.append(hist)\n",
    "\n",
    "    model_load = keras.models.load_model(model_chck_point_path)\n",
    "\n",
    "    # Evaluate the model on the test set for this fold and store the accuracy score\n",
    "    test_loss, test_acc = model_load.evaluate(test_pixel_array, test_label_array, verbose=0)\n",
    "    scores_acc_eval.append(test_acc)\n",
    "\n",
    "    y_pred = model_load.predict(test_pixel_array)\n",
    "    binary_predictions = np.round(y_pred)\n",
    "\n",
    "    f1 = f1_score(test_label_array, binary_predictions)\n",
    "    scores_f1.append(f1)\n",
    "\n",
    "    accuracy = accuracy_score(test_label_array, binary_predictions)\n",
    "    scores_acc_acc.append(accuracy)\n",
    "\n",
    "    report = classification_report(test_label_array, binary_predictions)\n",
    "    classification_reports.append(report)\n",
    "\n",
    "    matrix = confusion_matrix(test_label_array, binary_predictions)\n",
    "    confusion_matrices.append(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scores acc eval:\",scores_acc_eval)\n",
    "print(\"accuracy scores:\",scores_acc_acc)\n",
    "print(\"f1-scores:\",scores_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------ RUN 1 (fold 1) ------------------------\")\n",
    "print(classification_reports[0])\n",
    "print(\"------------------------ RUN 2 (fold 2) ------------------------\")\n",
    "print(classification_reports[1])\n",
    "print(\"------------------------ RUN 3 (fold 3) ------------------------\")\n",
    "print(classification_reports[2])\n",
    "print(\"------------------------ RUN 4 (fold 4) ------------------------\")\n",
    "print(classification_reports[3])\n",
    "print(\"------------------------ RUN 5 (fold 5) ------------------------\")\n",
    "print(classification_reports[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['occluded', 'recanalized']\n",
    "\n",
    "print(\"------------------------ RUN 1 (fold 1) ------------------------\")\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrices[0], display_labels=class_names)\n",
    "cmd.plot()\n",
    "print(\"------------------------ RUN 2 (fold 2) ------------------------\")\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrices[1], display_labels=class_names)\n",
    "cmd.plot()\n",
    "print(\"------------------------ RUN 3 (fold 3) ------------------------\")\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrices[2], display_labels=class_names)\n",
    "cmd.plot()\n",
    "print(\"------------------------ RUN 4 (fold 4) ------------------------\")\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrices[3], display_labels=class_names)\n",
    "cmd.plot()\n",
    "print(\"------------------------ RUN 5 (fold 5) ------------------------\")\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrices[4], display_labels=class_names)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(history_list[0].history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(history_list[0].history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(history_list[1].history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(history_list[1].history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(history_list[2].history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(history_list[2].history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(history_list[3].history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(history_list[3].history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,5), dpi=150)\n",
    "plt.grid('on')\n",
    "plt.title('Loss Function')\n",
    "plt.plot(history_list[4].history['loss'], 'b', lw=2, alpha=0.7, label='Training')\n",
    "plt.plot(history_list[4].history['val_loss'], 'r', lw=2, alpha=0.7, label='Val')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
